<<<<<<< HEAD
# If we add the cluster level to the database it would create difficulties
# in the analysis. We can treat it as a third dimension instead
data_long <- melt(data_scaled,
id.vars = c("Variable", "Cluster"),  # Keep these columns fixed
variable.name = "Date",  # Column name for the time periods
value.name = "Value")  # Column name for observations
View(data_long)
# Maximum number of clusters to test
n_clusters <- 50
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k - 1] <- mean(sil_info[, 3])  # Store average silhouette width
}
# Find the first k where improvement is small
k_star <- 2  # Start from the minimum valid k
for (k in 3:(n_clusters - 1)) {
diff_prev <- sil_scores[k - 2] - sil_scores[k - 3]  # Previous improvement
diff_curr <- sil_scores[k - 1] - sil_scores[k - 2]  # Current improvement
# Break if the improvement is less than a threshold (absolute or percentage-based)
if (diff_curr < 0.01 || (diff_prev > 0 && (diff_curr / diff_prev) < 0.05)) {
k_star <- k
break
}
}
# Maximum number of clusters to test
n_clusters <- 50
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
k_star <- 2  # Default minimum cluster number
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k - 1] <- mean(sil_info[, 3])  # Store average silhouette width
# Apply stopping rule for diminishing returns (only after k=3)
if (k > 2) {
diff_prev <- sil_scores[k - 2] - sil_scores[k - 3]  # Previous improvement
diff_curr <- sil_scores[k - 1] - sil_scores[k - 2]  # Current improvement
# Ensure values are valid before applying the rule
if (!is.na(diff_curr) && !is.na(diff_prev)) {
if (diff_curr < 0.01 || (diff_prev > 0 && (diff_curr / diff_prev) < 0.05)) {
k_star <- k
break  # Stop looping when improvement is too small
}
}
}
}
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k - 1] <- mean(sil_info[, 3])  # Store average silhouette width
# Apply stopping rule for diminishing returns (only after k=3)
if (k > 2) {
diff_curr <- sil_scores[k - 1] - sil_scores[k - 2]  # Current improvement
if (diff_curr < 0.01 || (diff_prev > 0 && (diff_curr / diff_prev) < 0.05)) {
k_star <- k
break  # Stop looping when improvement is too small
}
}
}
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k - 1] <- mean(sil_info[, 3])  # Store average silhouette width
# Apply stopping rule for diminishing returns (only after k=3)
if (k > 2) {
diff_curr <- sil_scores[k - 1] - sil_scores[k - 2]  # Current improvement
if (diff_curr < 0.01) {
k_star <- k
break  # Stop looping when improvement is too small
}
}
}
# Create a dataframe for visualization
sil_df <- tibble(clusters = 2:(1 + length(sil_scores)), silhouette = sil_scores)
# Plot silhouette scores
sil_plot <- ggplot(sil_df, aes(x = clusters, y = silhouette, group = 1)) +
geom_point(size = 4, color = "blue") +
geom_line(color = "blue") +
xlab("Number of Clusters (k)") +
ylab("Average Silhouette Score") +
ggtitle("Silhouette Scores for Different k") +
theme_minimal()
print(sil_plot)
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k - 1] <- mean(sil_info[, 3])  # Store average silhouette width
# Apply stopping rule for diminishing returns (only after k=3)
if (k > 2) {
diff_curr <- sil_scores[k - 1] - sil_scores[k - 2]  # Current improvement
if (diff_curr < 0.0001) {
k_star <- k
break  # Stop looping when improvement is too small
}
}
}
# Maximum number of clusters to test
n_clusters <- 50
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
k_star <- 2  # Default minimum cluster number
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k - 1] <- mean(sil_info[, 3])  # Store average silhouette width
# Apply stopping rule for diminishing returns (only after k=3)
if (k > 2) {
diff_curr <- sil_scores[k - 1] - sil_scores[k - 2]  # Current improvement
if (diff_curr < 0.0001) {
k_star <- k
break  # Stop looping when improvement is too small
}
}
}
# Create a dataframe for visualization
sil_df <- tibble(clusters = 2:(1 + length(sil_scores)), silhouette = sil_scores)
# Plot silhouette scores
sil_plot <- ggplot(sil_df(1:k_star,), aes(x = clusters, y = silhouette, group = 1)) +
geom_point(size = 4, color = "blue") +
geom_line(color = "blue") +
xlab("Number of Clusters (k)") +
ylab("Average Silhouette Score") +
ggtitle("Silhouette Scores for Different k") +
theme_minimal()
View(sil_df)
sil_info
# Maximum number of clusters to test
n_clusters <- 50
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
k_star <- 2  # Default minimum cluster number
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k - 1] <- mean(sil_info[, 3])  # Store average silhouette width
# Apply stopping rule for diminishing returns (only after k=3)
if (k > 2) {
diff_curr <- sil_scores[k - 1] - sil_scores[k - 2]  # Current improvement
if (diff_curr < 0.0001) {
k_star <- k
break  # Stop looping when improvement is too small
}
}
}
# Create a dataframe for visualization
sil_df <- tibble(clusters = 2:(1 + length(sil_scores)), silhouette = sil_scores)
# Plot silhouette scores
sil_plot <- ggplot(sil_df[1:k_star,], aes(x = clusters, y = silhouette, group = 1)) +
geom_point(size = 4, color = "blue") +
geom_line(color = "blue") +
xlab("Number of Clusters (k)") +
ylab("Average Silhouette Score") +
ggtitle("Silhouette Scores for Different k") +
theme_minimal()
print(sil_plot)
# Print optimal k
print(paste("Optimal number of clusters:", k_star))
# Add cluster labels
data_scaled$Cluster <- as.factor(kmeans_res$cluster)
View(data_scaled)
data_long <- melt(data_scaled,
id.vars = c("Variable", "Cluster"),  # Keep these columns fixed
variable.name = "Date",  # Column name for the time periods
value.name = "Value")  # Column name for observations
sil_info
which.max(sil_info)
which.max(sil_info$sil_width)
which.max(sil_info[,3])
source("C:/Users/HP/Downloads/UChicago/1. Courses/2. Winter Quarter 2025/2.3 MACSS 31330 Econometrics and Machine Learning/ECMA31330_MLProject/Code/data_analysis.R", echo=TRUE)
sil_scores
View(sil_df)
# Maximum number of clusters to test
n_clusters <- 50
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k - 1] <- mean(sil_info[, 3])  # Store average silhouette width
}
# Maximum number of clusters to test
n_clusters <- 10
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k - 1] <- mean(sil_info[, 3])  # Store average silhouette width
}
# Determine the optimal number of clusters
k_star <- which.max(sil_scores) + 1  # Add 1 since k starts from 2
# Create a dataframe for visualization
sil_df <- tibble(clusters = 2:n_clusters, silhouette = sil_scores)
# Plot silhouette scores
sil_plot <- ggplot(sil_df, aes(x = clusters, y = silhouette, group = 1)) +
geom_point(size = 4, color = "blue") +
geom_line(color = "blue") +
xlab("Number of Clusters (k)") +
ylab("Average Silhouette Score") +
ggtitle("Silhouette Scores for Different k") +
theme_minimal()
print(sil_plot)
# Print optimal k
print(paste("Optimal number of clusters:", k_star))
# Maximum number of clusters to test
n_clusters <- 20
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k - 1] <- mean(sil_info[, 3])  # Store average silhouette width
}
# Determine the optimal number of clusters
k_star <- which.max(sil_scores) + 1  # Add 1 since k starts from 2
# Create a dataframe for visualization
sil_df <- tibble(clusters = 2:n_clusters, silhouette = sil_scores)
# Plot silhouette scores
sil_plot <- ggplot(sil_df, aes(x = clusters, y = silhouette, group = 1)) +
geom_point(size = 4, color = "blue") +
geom_line(color = "blue") +
xlab("Number of Clusters (k)") +
ylab("Average Silhouette Score") +
ggtitle("Silhouette Scores for Different k") +
theme_minimal()
print(sil_plot)
# Print optimal k
print(paste("Optimal number of clusters:", k_star))
# Maximum number of clusters to test
n_clusters <- 10
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k - 1] <- mean(sil_info[, 3])  # Store average silhouette width
}
# Determine the optimal number of clusters
k_star <- which.max(sil_scores) + 1  # Add 1 since k starts from 2
# Create a dataframe for visualization
sil_df <- tibble(clusters = 2:n_clusters, silhouette = sil_scores)
# Plot silhouette scores
sil_plot <- ggplot(sil_df, aes(x = clusters, y = silhouette, group = 1)) +
geom_point(size = 4, color = "blue") +
geom_line(color = "blue") +
xlab("Number of Clusters (k)") +
ylab("Average Silhouette Score") +
ggtitle("Silhouette Scores for Different k") +
theme_minimal()
print(sil_plot)
# Print optimal k
print(paste("Optimal number of clusters:", k_star))
kmeans_res <- kmeans(data_scaled[, -1], centers = 2)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores <- mean(sil_info[, 3])  # Store average silhouette width
kmeans_res <- kmeans(data_scaled[, -1], centers = 3)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores <- mean(sil_info[, 3])  # Store average silhouette width
kmeans_res <- kmeans(data_scaled[, -1], centers = 10)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores <- mean(sil_info[, 3])  # Store average silhouette width
kmeans_res <- kmeans(data_scaled[, -1], centers = 12)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores <- mean(sil_info[, 3])  # Store average silhouette width
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
# Maximum number of clusters to test
n_clusters <- 10
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
k_star <- 0
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k - 1] <- mean(sil_info[, 3])  # Store average silhouette width
if (k > 2){
if (sil_scores[k-1] > sil_scores[k-2]){
k_star <- k
break
}
}
}
sil_scores
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k] <- mean(sil_info[, 3])  # Store average silhouette width
if (k > 2){
if (sil_scores[k] > sil_scores[k-1]){
k_star <- k
break
}
}
}
sil_scores
# Maximum number of clusters to test
n_clusters <- 10
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
k_star <- 0
# Loop through different cluster numbers
for (k in 2:n_clusters) {
kmeans_res <- kmeans(data_scaled[, -1], centers = k)  # Run k-means
# Compute silhouette score
sil_info <- silhouette(kmeans_res$cluster, dist(data_scaled[, -1]))
sil_scores[k] <- mean(sil_info[, 3])  # Store average silhouette width
# if (k > 2){
#   if (sil_scores[k] > sil_scores[k-1]){
#     k_star <- k
#     break
#   }
# }
}
sil_scores
# Determine the optimal number of clusters
k_star <- which.max(sil_scores)
# Create a dataframe for visualization
sil_df <- tibble(clusters = 2:n_clusters, silhouette = sil_scores)
# Create a dataframe for visualization
sil_df <- tibble(clusters = n_clusters, silhouette = sil_scores)
# Plot silhouette scores
sil_plot <- ggplot(sil_df, aes(x = clusters, y = silhouette, group = 1)) +
geom_point(size = 4, color = "blue") +
geom_line(color = "blue") +
xlab("Number of Clusters (k)") +
ylab("Average Silhouette Score") +
ggtitle("Silhouette Scores for Different k") +
theme_minimal()
print(sil_plot)
View(sq_df)
View(sil_df)
# Create a dataframe for visualization
sil_df <- tibble(clusters = 1:n_clusters, silhouette = sil_scores)
# Plot silhouette scores
sil_plot <- ggplot(sil_df, aes(x = clusters, y = silhouette, group = 1)) +
geom_point(size = 4, color = "blue") +
geom_line(color = "blue") +
xlab("Number of Clusters (k)") +
ylab("Average Silhouette Score") +
ggtitle("Silhouette Scores for Different k") +
theme_minimal()
print(sil_plot)
# Plot silhouette scores
sil_plot <- ggplot(sil_df, aes(x = clusters, y = silhouette, group = 1)) +
geom_point(size = 4, color = "blue") +
geom_line(color = "blue") +
xlab("Number of Clusters (k)") +
ylab("Average Silhouette Score") +
scale_x_continuous()
# Plot silhouette scores
sil_plot <- ggplot(sil_df, aes(x = clusters, y = silhouette, group = 1)) +
geom_point(size = 4, color = "blue") +
geom_line(color = "blue") +
xlab("Number of Clusters (k)") +
ylab("Average Silhouette Score") +
scale_x_continuous() +
ggtitle("Silhouette Scores for Different k") +
theme_minimal()
print(sil_plot)
# Print optimal k
print(paste("Optimal number of clusters:", k_star))
il_df <- tibble(clusters = 1:n_clusters, silhouette = sil_scores)
# Plot silhouette scores
sil_plot <- ggplot(sil_df, aes(x = clusters, y = silhouette, group = 1)) +
geom_point(size = 4, color = "blue") +
geom_line(color = "blue") +
xlab("Number of Clusters (k)") +
ylab("Average Silhouette Score") +
scale_x_continuous() +
ggtitle("Silhouette Scores for Different k") +
theme_minimal()
print(sil_plot)
# Plot silhouette scores
sil_plot <- ggplot(sil_df, aes(x = clusters, y = silhouette, group = 1)) +
geom_point(size = 4, color = "blue") +
geom_line(color = "blue") +
xlab("Number of Clusters (k)") +
ylab("Average Silhouette Score") +
scale_x_continuous(breaks = seq(1, n_clusters, by = 1)) +
ggtitle("Silhouette Scores for Different k") +
theme_minimal()
print(sil_plot)
# Add cluster labels
data_scaled$Cluster <- as.factor(kmeans_res$cluster)
data_long <- melt(data_scaled,
id.vars = c("Variable", "Cluster"),  # Keep these columns fixed
variable.name = "Date",  # Column name for the time periods
value.name = "Value")  # Column name for observations
# Maximum number of clusters to test
n_clusters <- 10
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
k_star <- 0
# Loop through different cluster numbers
for (k in 2:n_clusters) {
tsclust_res <- tsclust(data_scaled[, -1], k = k, distance = "dtw", type = "partitional")  # DTW-based clustering
# Compute dissimilarity matrix
diss_matrix <- proxy::dist(data_scaled[, -1], method = "DTW")
# Compute silhouette score
sil_info <- silhouette(tsclust_res@cluster, diss_matrix)
sil_scores[k] <- mean(sil_info[, 3])  # Store average silhouette width
}
data_kmeans_dyn <- data_scaled
# Maximum number of clusters to test
n_clusters <- 10
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
k_star <- 0
# Loop through different cluster numbers
for (k in 2:n_clusters) {
tsclust_res <- tsclust(data_kmeans_dyn[, -1], k = k, distance = "dtw", type = "partitional")  # DTW-based clustering
# Compute dissimilarity matrix
diss_matrix <- proxy::dist(data_kmeans_dyn[, -1], method = "DTW")
# Compute silhouette score
sil_info <- silhouette(tsclust_res@cluster, diss_matrix)
sil_scores[k] <- mean(sil_info[, 3])  # Store average silhouette width
}
View(data_kmeans_dyn)
# USING DATA "1998_G7_US"
data <- fread("1998_G7_US.csv", na.strings = "NA")
# We need to format the date for future use
data <- data %>%
mutate(date = seq.Date(from = as.Date("1998-01-01"), by = "month", length.out = n()))
# Take away US monetary policy
data <- data %>% select(-US)
# Scale the numeric data
data_no_date <- data[, -1]
data_scaled <- scale(data_no_date[, lapply(.SD, as.numeric), .SDcols = names(data_no_date)])
#DATA RESTRUCTURE
# I need to transpose the data so the cluster is on variables and not on dates
data_scaled <- as.data.table(t(data_scaled))
# Assign dates to colnames
setnames(data_scaled, old = names(data_scaled), new = as.character(data[[1]]))  # Use dates as column names
# Add a new column for variable names
data_scaled[, Variable := colnames(data_no_date)]
# Move Variable column to the first position
setcolorder(data_scaled, c("Variable", setdiff(names(data_scaled), "Variable")))
data_kmeans_dyn <- data_scaled
# Maximum number of clusters to test
n_clusters <- 10
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
k_star <- 0
# Loop through different cluster numbers
for (k in 2:n_clusters) {
tsclust_res <- tsclust(data_kmeans_dyn[, -1], k = k, distance = "dtw", type = "partitional")  # DTW-based clustering
# Compute dissimilarity matrix
diss_matrix <- proxy::dist(data_kmeans_dyn[, -1], method = "DTW")
# Compute silhouette score
sil_info <- silhouette(tsclust_res@cluster, diss_matrix)
sil_scores[k] <- mean(sil_info[, 3])  # Store average silhouette width
}
data_kmeans_dyn <- data_scaled
# Maximum number of clusters to test
n_clusters <- 10
# Initialize silhouette scores
sil_scores <- numeric(n_clusters - 1)  # k must be at least 2
k_star <- 0
View(data_kmeans_dyn)
# Loop through different cluster numbers
for (k in 2:n_clusters) {
tsclust_res <- tsclust(data_kmeans_dyn[,-1], type = "partitional", k = k,
distance = "L2", centroid = "dtw", seed = 123)  # DTW-based clustering
# Compute dissimilarity matrix
diss_matrix <- proxy::dist(data_kmeans_dyn[, -1], method = "DTW")
# Compute silhouette score
sil_info <- silhouette(tsclust_res@cluster, diss_matrix)
sil_scores[k] <- mean(sil_info[, 3])  # Store average silhouette width
}
# Loop through different cluster numbers
for (k in 2:n_clusters) {
tsclust_res <- tsclust(data_kmeans_dyn[,-1], type = "partitional", k = k,
distance = "L2", centroid = "sdtw_cent", seed = 123)  # DTW-based clustering
# Compute dissimilarity matrix
diss_matrix <- proxy::dist(data_kmeans_dyn[, -1], method = "DTW")
# Compute silhouette score
sil_info <- silhouette(tsclust_res@cluster, diss_matrix)
sil_scores[k] <- mean(sil_info[, 3])  # Store average silhouette width
}
=======
return(which.min(distances))
})
}
# Run the function and set a new column with the cluster prediction
test_df$cluster_pred <- assign_clusters(test_df_numeric, centroids)
# majority_rule$jobclass holds the jobclass of each cluster;
# predicting the observations' jobclass based on the predicted cluster's jobclass:
test_df$jobclass_pred <- majority_rule$jobclass[test_df$cluster_pred]
correctly_identified <- sum(test_df$jobclass == test_df$jobclass_pred,
na.rm = TRUE)
test_accuracy_kmeans <- correctly_identified/nrow(test_df)*100
cat("Classification accuracy in the test set",
sprintf("%.2f%%", test_accuracy_kmeans), "\n")
test_df <- df[!sample, ]
# Load libraries
library(ISLR2)
library(dplyr)
library(keras)
library(torch)
library(luz) # high-level interface for torch
library(zeallot)
library(rpart)
library(rpart.plot)
library(knitr)
set.seed(123)
knitr::opts_chunk$set(comment = NA)
#Load Dataset
wage_data <- Wage # From the ISLR2 Package: https://rdrr.io/cran/ISLR2/man/Wage.html
# Convert the 'jobclass' variable to binary: 1 for Information, 0 for Industrial
wage_data$jobclass <- ifelse(wage_data$jobclass == "2. Information", 1, 0)
kable(head(wage_data[, 1:7]))
n <- nrow(wage_data)
sample <- sample(seq_len(n), size = 0.8 * n)
# Will implement actual subset after the data cleaning portion is complete:
#train_df  <- wage_data[sample, ]
#test_df  <- wage_data[!sample, ]
education_mapping <- c(
"< HS Grad" = 9,
"HS Grad" = 12,
"Some College" = 14,
"College Grad" = 16,
"Advanced Degree" = 18
)
wage_data$education_years <- education_mapping[wage_data$education]
df <- wage_data[c("jobclass", "age", "logwage", "education_years")]
df[c(2:4)] <- lapply(df[c(2:4)], function(x) c(scale(x, center = TRUE,
scale = TRUE)))
# Split data into train/test split
train_df <- df[sample, ]
test_df <- df[-sample, ]
# Running it w/o the jobclass variable
kmeans_results <- kmeans(train_df[, -c(1)], centers = 8, nstart = 35)
train_df$cluster <- kmeans_results$cluster
majority_rule <- aggregate(jobclass ~ cluster, data = train_df,
FUN = function(x) as.integer(mean(x) > 0.5))
# Assign majority rule jobclass predictions to each observation
train_df$jobclass_majority_pred <- majority_rule$jobclass[match(train_df$cluster,
majority_rule$cluster)]
correct_majority <- sum(train_df$jobclass == train_df$jobclass_majority_pred,
na.rm = TRUE)
train_accuracy_kmeans <- correct_majority/nrow(train_df)*100
cat("Within-sample classification accuracy using Majority Rule:",
sprintf("%.2f%%", train_accuracy_kmeans), "\n")
centroids <- kmeans_results$centers
# Get the numeric variables of the test_df only
test_df_numeric <- test_df[, -which(colnames(test_df) == "jobclass")]
# Define a function that takes each row and calculates the distance between it
# and the centroids, then determines the one closest to the observation:
assign_clusters <- function(data, centroids) {
apply(data, 1, function(row) {
distances <- apply(centroids, 1, function(centroid) sum((row - centroid)^2))
return(which.min(distances))
})
}
# Run the function and set a new column with the cluster prediction
test_df$cluster_pred <- assign_clusters(test_df_numeric, centroids)
# majority_rule$jobclass holds the jobclass of each cluster;
# predicting the observations' jobclass based on the predicted cluster's jobclass:
test_df$jobclass_pred <- majority_rule$jobclass[test_df$cluster_pred]
correctly_identified <- sum(test_df$jobclass == test_df$jobclass_pred,
na.rm = TRUE)
test_accuracy_kmeans <- correctly_identified/nrow(test_df)*100
cat("Classification accuracy in the test set",
sprintf("%.2f%%", test_accuracy_kmeans), "\n")
# Keep train and test df as is but drop columns from other methods:
train_df_rt  <- wage_data[sample, c("age", "logwage", "education_years",
"jobclass")]
test_df_rt  <- wage_data[-sample,c("age", "logwage", "education_years",
"jobclass")]
# Fit the regression tree with train data
# First we create a larger tree and then prune it as in discussion lab
model <- rpart(jobclass ~ age + logwage + education_years,
data = train_df_rt,
method = "class",  # Regression tree
control = rpart.control(maxdepth = 4,
cp = 0.001   # small cp for a big initial tree
))
# Prune tree using the number of nodes desired (8) A tree with 8 nodes has 7 splits
opt_cp <- model$cptable[which(model$cptable[,"nsplit"] == 7), "CP"]
pruned_tree <- prune(model, cp = opt_cp)
# Plot the tree
rpart.plot(pruned_tree,
type = 2,               # Use 2 for standard dendrogram
extra = 106,            # Show all information
box.palette = "GnBu",   # Green-Blue palette
shadow.col = "gray",    # Add shadows
fallen.leaves = TRUE,   # Align terminal nodes
main = "Wage Dataset Regression Tree")
# Prediction on the train data
train_pred <- predict(pruned_tree, train_df_rt, type = "class")
# Calculate accuracy for the training data
train_accuracy_rt <- mean(train_pred == train_df_rt$jobclass)
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rt * 100), "\n")
# Predicting on the test data
test_pred <- predict(pruned_tree, test_df_rt, type = "class")
# Calculate accuracy for the test data
test_accuracy_rt <- mean(test_pred == test_df_rt$jobclass)
cat("Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rt * 100), "\n")
# Creating the full training dataset, education (5, categorical variable)
# because we have education_years and dropping wage (11) because we have logwage
train_df_rt2 <- wage_data[sample, -c(5, 11)]
test_df_rt2  <- wage_data[-sample,-c(5, 11)]
model_all <- rpart(jobclass ~ .,
data = train_df_rt2,
method = "class",  # Regression tree
control = rpart.control(maxdepth = 4,
cp = 0.001   # small cp for a big initial tree
))
# Prune tree using the number of nodes desired (8) A tree with 8 nodes has 7 splits
opt_cp_all <- model_all$cptable[which(model_all$cptable[,"nsplit"] == 7), "CP"]
pruned_tree_all <- prune(model_all, cp = opt_cp_all)
rpart.plot(pruned_tree_all,
type = 2,               # Use 2 for standard dendrogram
extra = 106,            # Show all information
box.palette = "GnBu",   # Green-Blue palette
shadow.col = "gray",    # Add shadows
fallen.leaves = TRUE,   # Align terminal nodes
main = "Wage Dataset Regression Tree")
# Prediction on the train data
train_pred_all <- predict(pruned_tree_all, train_df_rt2, type = "class")
# Calculate accuracy for the training data
train_accuracy_rt_all <- mean(train_pred_all == train_df_rt2$jobclass)
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rt_all * 100), "\n")
# Prediction on the test data
test_pred_all <- predict(pruned_tree_all, test_df_rt2, type = "class")
# Calculate accuracy for the test data
test_accuracy_rt_all <- mean(test_pred_all == test_df_rt2$jobclass)
cat("Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rt_all * 100), "\n")
train_nn_data <- train_df[, c("age", "logwage", "education_years", "jobclass")]
test_nn_data <- test_df[, c("age", "logwage", "education_years", "jobclass")]
write.csv(train_nn_data, "train_nn.csv" )
write.csv(test_nn_data, "test_nn.csv" )
# Prediction on the test data
test_pred_all <- predict(pruned_tree_all, test_df_rt2, type = "class")
# Calculate accuracy for the test data
test_accuracy_rt_all <- mean(test_pred_all == test_df_rt2$jobclass)*100
cat("Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rt_all), "\n")
# Prediction on the train data
train_pred_all <- predict(pruned_tree_all, train_df_rt2, type = "class")
# Calculate accuracy for the training data
train_accuracy_rt_all <- mean(train_pred_all == train_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rt_all, "\n")
# Prediction on the train data
train_pred_all <- predict(pruned_tree_all, train_df_rt2, type = "class")
# Calculate accuracy for the training data
train_accuracy_rt_all <- mean(train_pred_all == train_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rt_all), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ age + logwage + education_years,
data = train_df_rt,
ntree = 500,    # Number of trees
mtry = 2,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Print model summary
print(rf_model)
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ age + logwage + education_years,
data = train_df_rt,
ntree = 500,    # Number of trees
mtry = 2,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ age + logwage + education_years,
data = train_df_rt,
ntree = 500,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ age + logwage + education_years,
data = train_df_rt,
ntree = 500,    # Number of trees
mtry = 4,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ age + logwage + education_years,
data = train_df_rt,
ntree = 500,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ age + logwage + education_years,
data = train_df_rt,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
train_df_rt2$jobclass <- as.factor(train_df_rt2$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
train_df_rt2$jobclass <- as.factor(train_df_rt2$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt2)
test_predictions <- predict(rf_model, test_df_rt2)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt2$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
train_df_rt2$jobclass <- as.factor(train_df_rt2$jobclass)
# Train the random forest model
rf_model2 <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions2 <- predict(rf_model2, train_df_rt2)
test_predictions2 <- predict(rf_model2, test_df_rt2)
# Calculate accuracy for training data
train_accuracy_rf2 <- mean(train_predictions2 == train_df_rt2$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf2 <- mean(test_predictions2 == test_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf2),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf2), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ .,
data = train_df_rt,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ .,
data = train_df_rt,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf), "\n",
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
train_df_rt2$jobclass <- as.factor(train_df_rt2$jobclass)
# Train the random forest model
rf_model2 <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions2 <- predict(rf_model2, train_df_rt2)
test_predictions2 <- predict(rf_model2, test_df_rt2)
# Calculate accuracy for training data
train_accuracy_rf2 <- mean(train_predictions2 == train_df_rt2$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf2 <- mean(test_predictions2 == test_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf2),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf2), "\n")
train_df_rt2$jobclass <- as.factor(train_df_rt2$jobclass)
# Train the random forest model
rf_model2 <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions2 <- predict(rf_model2, train_df_rt2)
test_predictions2 <- predict(rf_model2, test_df_rt2)
# Calculate accuracy for training data
train_accuracy_rf2 <- mean(train_predictions2 == train_df_rt2$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf2 <- mean(test_predictions2 == test_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf2), "\n",
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf2), "\n")
train_df_rt2$jobclass <- as.factor(train_df_rt2$jobclass)
# Train the random forest model
rf_model2 <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions2 <- predict(rf_model2, train_df_rt2)
test_predictions2 <- predict(rf_model2, test_df_rt2)
# Calculate accuracy for training data
train_accuracy_rf2 <- mean(train_predictions2 == train_df_rt2$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf2 <- mean(test_predictions2 == test_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf2), "\n",
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf2), "\n")
#     Nour Abdelbaki & Giuliana Triberti
## In this code, we are cross-validating our random forests for each time period.
# Then, running the model and saving the model object to use for our analysis.
# Import Libraries
library(dplyr)
library(tidyr)
library(purrr)
library(randomForest)
# Set seed for reproducibility
set.seed(123)
## Set working directory
setwd("~/Desktop/MACSS-Econ/Winter 2025/ECMA 31330/ECMA31330_MLProject")
## Read dataset
data <- read.csv("1998_G7_US.csv")
p <- dim(data)[2] - 1 #Total number of features w/o US data
n <- dim(data)[1]
hyper_grid <- expand.grid(
mtry = floor(p * c(.05, .15, .25, .333, .4)),
nodesize = c(3, 5, 10, 15),
replace = c(TRUE, FALSE),
sampsize = floor(n* c(.5, .63, .8)),
rmse = NA
)
datab4_08 <- data[data$date < as.yearmon("Dec 2007", "%b %Y"),]
# Import Libraries
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(randomForest)
datab4_08 <- data[data$date < as.yearmon("Dec 2007", "%b %Y"),]
library(lubridate)
datab4_08 <- data[data$date < as.yearmon("Dec 2007", "%b %Y"),]
datab4_08 <- data[data$date < lubridate::as.yearmon("Dec 2007", "%b %Y"),]
# Import Libraries
library(dplyr)
library(tidyr)
library(zoo)
library(randomForest)
Datasets:
# Officially the Great Recession/2008 Financial Crisis lasted from Dec 2007
# to June 2009. We chose the after '08 dataset to also end before the
# COVID-19 crisis.
datab4_08 <- data[data$date < as.yearmon("Dec 2007", "%b %Y"),]
datab4_08 <- data[data$date < as.yearmon("Dec 2007", "%b %Y"),]
dataAfter08 <- data[data$date >=  as.yearmon("Dec 2007", "%b %Y") &
data$date < as.yearmon("March 2020", "%b %Y"), ]
#### Cross-validation before 2008:
n_08 <- dim(datab4_08)[1]
hyper_grid_b4_08 <- expand.grid(
mtry = floor(p * c(.05, .15, .25, .333, .4)),
nodesize = c(3, 5, 10, 15),
replace = c(TRUE, FALSE),
sampsize = floor(n_08* c(.5, .63, .8)),
rmse = NA
)
### TAKES AROUND 4 MINS TO RUN:
for(i in seq_len(nrow(hyper_grid_b4_08))) {
# fit model for ith hyperparameter combination
fit <- randomForest(formula= US ~ ., data = datab4_08, ntree = p * 10,
mtry = hyper_grid_b4_08$mtry[i],
nodesize = hyper_grid_b4_08$nodesize[i],
replace = hyper_grid_b4_08$replace[i],
sampsize = hyper_grid_b4_08$sampsize[i]
)
# save rmse
hyper_grid_b4_08$rmse[i] <- sqrt(fit$mse)
}
hyper_grid_b4_08 <- hyper_grid_b4_08 %>%
arrange(rmse)
head(hyper_grid_b4_08, 10)
>>>>>>> 01f60b5aa059c4e3a71d1aa63bb199de8b9a9073
