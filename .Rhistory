return(which.min(distances))
})
}
# Run the function and set a new column with the cluster prediction
test_df$cluster_pred <- assign_clusters(test_df_numeric, centroids)
# majority_rule$jobclass holds the jobclass of each cluster;
# predicting the observations' jobclass based on the predicted cluster's jobclass:
test_df$jobclass_pred <- majority_rule$jobclass[test_df$cluster_pred]
correctly_identified <- sum(test_df$jobclass == test_df$jobclass_pred,
na.rm = TRUE)
test_accuracy_kmeans <- correctly_identified/nrow(test_df)*100
cat("Classification accuracy in the test set",
sprintf("%.2f%%", test_accuracy_kmeans), "\n")
test_df <- df[!sample, ]
# Load libraries
library(ISLR2)
library(dplyr)
library(keras)
library(torch)
library(luz) # high-level interface for torch
library(zeallot)
library(rpart)
library(rpart.plot)
library(knitr)
set.seed(123)
knitr::opts_chunk$set(comment = NA)
#Load Dataset
wage_data <- Wage # From the ISLR2 Package: https://rdrr.io/cran/ISLR2/man/Wage.html
# Convert the 'jobclass' variable to binary: 1 for Information, 0 for Industrial
wage_data$jobclass <- ifelse(wage_data$jobclass == "2. Information", 1, 0)
kable(head(wage_data[, 1:7]))
n <- nrow(wage_data)
sample <- sample(seq_len(n), size = 0.8 * n)
# Will implement actual subset after the data cleaning portion is complete:
#train_df  <- wage_data[sample, ]
#test_df  <- wage_data[!sample, ]
education_mapping <- c(
"< HS Grad" = 9,
"HS Grad" = 12,
"Some College" = 14,
"College Grad" = 16,
"Advanced Degree" = 18
)
wage_data$education_years <- education_mapping[wage_data$education]
df <- wage_data[c("jobclass", "age", "logwage", "education_years")]
df[c(2:4)] <- lapply(df[c(2:4)], function(x) c(scale(x, center = TRUE,
scale = TRUE)))
# Split data into train/test split
train_df <- df[sample, ]
test_df <- df[-sample, ]
# Running it w/o the jobclass variable
kmeans_results <- kmeans(train_df[, -c(1)], centers = 8, nstart = 35)
train_df$cluster <- kmeans_results$cluster
majority_rule <- aggregate(jobclass ~ cluster, data = train_df,
FUN = function(x) as.integer(mean(x) > 0.5))
# Assign majority rule jobclass predictions to each observation
train_df$jobclass_majority_pred <- majority_rule$jobclass[match(train_df$cluster,
majority_rule$cluster)]
correct_majority <- sum(train_df$jobclass == train_df$jobclass_majority_pred,
na.rm = TRUE)
train_accuracy_kmeans <- correct_majority/nrow(train_df)*100
cat("Within-sample classification accuracy using Majority Rule:",
sprintf("%.2f%%", train_accuracy_kmeans), "\n")
centroids <- kmeans_results$centers
# Get the numeric variables of the test_df only
test_df_numeric <- test_df[, -which(colnames(test_df) == "jobclass")]
# Define a function that takes each row and calculates the distance between it
# and the centroids, then determines the one closest to the observation:
assign_clusters <- function(data, centroids) {
apply(data, 1, function(row) {
distances <- apply(centroids, 1, function(centroid) sum((row - centroid)^2))
return(which.min(distances))
})
}
# Run the function and set a new column with the cluster prediction
test_df$cluster_pred <- assign_clusters(test_df_numeric, centroids)
# majority_rule$jobclass holds the jobclass of each cluster;
# predicting the observations' jobclass based on the predicted cluster's jobclass:
test_df$jobclass_pred <- majority_rule$jobclass[test_df$cluster_pred]
correctly_identified <- sum(test_df$jobclass == test_df$jobclass_pred,
na.rm = TRUE)
test_accuracy_kmeans <- correctly_identified/nrow(test_df)*100
cat("Classification accuracy in the test set",
sprintf("%.2f%%", test_accuracy_kmeans), "\n")
# Keep train and test df as is but drop columns from other methods:
train_df_rt  <- wage_data[sample, c("age", "logwage", "education_years",
"jobclass")]
test_df_rt  <- wage_data[-sample,c("age", "logwage", "education_years",
"jobclass")]
# Fit the regression tree with train data
# First we create a larger tree and then prune it as in discussion lab
model <- rpart(jobclass ~ age + logwage + education_years,
data = train_df_rt,
method = "class",  # Regression tree
control = rpart.control(maxdepth = 4,
cp = 0.001   # small cp for a big initial tree
))
# Prune tree using the number of nodes desired (8) A tree with 8 nodes has 7 splits
opt_cp <- model$cptable[which(model$cptable[,"nsplit"] == 7), "CP"]
pruned_tree <- prune(model, cp = opt_cp)
# Plot the tree
rpart.plot(pruned_tree,
type = 2,               # Use 2 for standard dendrogram
extra = 106,            # Show all information
box.palette = "GnBu",   # Green-Blue palette
shadow.col = "gray",    # Add shadows
fallen.leaves = TRUE,   # Align terminal nodes
main = "Wage Dataset Regression Tree")
# Prediction on the train data
train_pred <- predict(pruned_tree, train_df_rt, type = "class")
# Calculate accuracy for the training data
train_accuracy_rt <- mean(train_pred == train_df_rt$jobclass)
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rt * 100), "\n")
# Predicting on the test data
test_pred <- predict(pruned_tree, test_df_rt, type = "class")
# Calculate accuracy for the test data
test_accuracy_rt <- mean(test_pred == test_df_rt$jobclass)
cat("Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rt * 100), "\n")
# Creating the full training dataset, education (5, categorical variable)
# because we have education_years and dropping wage (11) because we have logwage
train_df_rt2 <- wage_data[sample, -c(5, 11)]
test_df_rt2  <- wage_data[-sample,-c(5, 11)]
model_all <- rpart(jobclass ~ .,
data = train_df_rt2,
method = "class",  # Regression tree
control = rpart.control(maxdepth = 4,
cp = 0.001   # small cp for a big initial tree
))
# Prune tree using the number of nodes desired (8) A tree with 8 nodes has 7 splits
opt_cp_all <- model_all$cptable[which(model_all$cptable[,"nsplit"] == 7), "CP"]
pruned_tree_all <- prune(model_all, cp = opt_cp_all)
rpart.plot(pruned_tree_all,
type = 2,               # Use 2 for standard dendrogram
extra = 106,            # Show all information
box.palette = "GnBu",   # Green-Blue palette
shadow.col = "gray",    # Add shadows
fallen.leaves = TRUE,   # Align terminal nodes
main = "Wage Dataset Regression Tree")
# Prediction on the train data
train_pred_all <- predict(pruned_tree_all, train_df_rt2, type = "class")
# Calculate accuracy for the training data
train_accuracy_rt_all <- mean(train_pred_all == train_df_rt2$jobclass)
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rt_all * 100), "\n")
# Prediction on the test data
test_pred_all <- predict(pruned_tree_all, test_df_rt2, type = "class")
# Calculate accuracy for the test data
test_accuracy_rt_all <- mean(test_pred_all == test_df_rt2$jobclass)
cat("Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rt_all * 100), "\n")
train_nn_data <- train_df[, c("age", "logwage", "education_years", "jobclass")]
test_nn_data <- test_df[, c("age", "logwage", "education_years", "jobclass")]
write.csv(train_nn_data, "train_nn.csv" )
write.csv(test_nn_data, "test_nn.csv" )
# Prediction on the test data
test_pred_all <- predict(pruned_tree_all, test_df_rt2, type = "class")
# Calculate accuracy for the test data
test_accuracy_rt_all <- mean(test_pred_all == test_df_rt2$jobclass)*100
cat("Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rt_all), "\n")
# Prediction on the train data
train_pred_all <- predict(pruned_tree_all, train_df_rt2, type = "class")
# Calculate accuracy for the training data
train_accuracy_rt_all <- mean(train_pred_all == train_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rt_all, "\n")
# Prediction on the train data
train_pred_all <- predict(pruned_tree_all, train_df_rt2, type = "class")
# Calculate accuracy for the training data
train_accuracy_rt_all <- mean(train_pred_all == train_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rt_all), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ age + logwage + education_years,
data = train_df_rt,
ntree = 500,    # Number of trees
mtry = 2,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Print model summary
print(rf_model)
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ age + logwage + education_years,
data = train_df_rt,
ntree = 500,    # Number of trees
mtry = 2,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ age + logwage + education_years,
data = train_df_rt,
ntree = 500,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ age + logwage + education_years,
data = train_df_rt,
ntree = 500,    # Number of trees
mtry = 4,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ age + logwage + education_years,
data = train_df_rt,
ntree = 500,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ age + logwage + education_years,
data = train_df_rt,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
train_df_rt2$jobclass <- as.factor(train_df_rt2$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
train_df_rt2$jobclass <- as.factor(train_df_rt2$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt2)
test_predictions <- predict(rf_model, test_df_rt2)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt2$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
train_df_rt2$jobclass <- as.factor(train_df_rt2$jobclass)
# Train the random forest model
rf_model2 <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions2 <- predict(rf_model2, train_df_rt2)
test_predictions2 <- predict(rf_model2, test_df_rt2)
# Calculate accuracy for training data
train_accuracy_rf2 <- mean(train_predictions2 == train_df_rt2$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf2 <- mean(test_predictions2 == test_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf2),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf2), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ .,
data = train_df_rt,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
library(randomForest)
# Turn jobclass into a factor (binary classification)
train_df_rt$jobclass <- as.factor(train_df_rt$jobclass)
# Train the random forest model
rf_model <- randomForest(jobclass ~ .,
data = train_df_rt,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions <- predict(rf_model, train_df_rt)
test_predictions <- predict(rf_model, test_df_rt)
# Calculate accuracy for training data
train_accuracy_rf <- mean(train_predictions == train_df_rt$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf <- mean(test_predictions == test_df_rt$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf), "\n",
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf), "\n")
train_df_rt2$jobclass <- as.factor(train_df_rt2$jobclass)
# Train the random forest model
rf_model2 <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions2 <- predict(rf_model2, train_df_rt2)
test_predictions2 <- predict(rf_model2, test_df_rt2)
# Calculate accuracy for training data
train_accuracy_rf2 <- mean(train_predictions2 == train_df_rt2$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf2 <- mean(test_predictions2 == test_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf2),
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf2), "\n")
train_df_rt2$jobclass <- as.factor(train_df_rt2$jobclass)
# Train the random forest model
rf_model2 <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions2 <- predict(rf_model2, train_df_rt2)
test_predictions2 <- predict(rf_model2, test_df_rt2)
# Calculate accuracy for training data
train_accuracy_rf2 <- mean(train_predictions2 == train_df_rt2$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf2 <- mean(test_predictions2 == test_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf2), "\n",
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf2), "\n")
train_df_rt2$jobclass <- as.factor(train_df_rt2$jobclass)
# Train the random forest model
rf_model2 <- randomForest(jobclass ~ .,
data = train_df_rt2,
ntree = 1000,    # Number of trees
mtry = 3,       # Number of features considered per split
importance = TRUE)  # Compute variable importance
# Run predictions
train_predictions2 <- predict(rf_model2, train_df_rt2)
test_predictions2 <- predict(rf_model2, test_df_rt2)
# Calculate accuracy for training data
train_accuracy_rf2 <- mean(train_predictions2 == train_df_rt2$jobclass)*100
# Calculate accuracy for test data
test_accuracy_rf2 <- mean(test_predictions2 == test_df_rt2$jobclass)*100
cat("Classification Accuracy for Training Dataset:",
sprintf("%.2f%%", train_accuracy_rf2), "\n",
"And Classification Accuracy for Test Dataset:",
sprintf("%.2f%%", test_accuracy_rf2), "\n")
#     Nour Abdelbaki & Giuliana Triberti
## In this code, we are cross-validating our random forests for each time period.
# Then, running the model and saving the model object to use for our analysis.
# Import Libraries
library(dplyr)
library(tidyr)
library(purrr)
library(randomForest)
# Set seed for reproducibility
set.seed(123)
## Set working directory
setwd("~/Desktop/MACSS-Econ/Winter 2025/ECMA 31330/ECMA31330_MLProject")
## Read dataset
data <- read.csv("1998_G7_US.csv")
p <- dim(data)[2] - 1 #Total number of features w/o US data
n <- dim(data)[1]
hyper_grid <- expand.grid(
mtry = floor(p * c(.05, .15, .25, .333, .4)),
nodesize = c(3, 5, 10, 15),
replace = c(TRUE, FALSE),
sampsize = floor(n* c(.5, .63, .8)),
rmse = NA
)
datab4_08 <- data[data$date < as.yearmon("Dec 2007", "%b %Y"),]
# Import Libraries
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(randomForest)
datab4_08 <- data[data$date < as.yearmon("Dec 2007", "%b %Y"),]
library(lubridate)
datab4_08 <- data[data$date < as.yearmon("Dec 2007", "%b %Y"),]
datab4_08 <- data[data$date < lubridate::as.yearmon("Dec 2007", "%b %Y"),]
# Import Libraries
library(dplyr)
library(tidyr)
library(zoo)
library(randomForest)
Datasets:
# Officially the Great Recession/2008 Financial Crisis lasted from Dec 2007
# to June 2009. We chose the after '08 dataset to also end before the
# COVID-19 crisis.
datab4_08 <- data[data$date < as.yearmon("Dec 2007", "%b %Y"),]
datab4_08 <- data[data$date < as.yearmon("Dec 2007", "%b %Y"),]
dataAfter08 <- data[data$date >=  as.yearmon("Dec 2007", "%b %Y") &
data$date < as.yearmon("March 2020", "%b %Y"), ]
#### Cross-validation before 2008:
n_08 <- dim(datab4_08)[1]
hyper_grid_b4_08 <- expand.grid(
mtry = floor(p * c(.05, .15, .25, .333, .4)),
nodesize = c(3, 5, 10, 15),
replace = c(TRUE, FALSE),
sampsize = floor(n_08* c(.5, .63, .8)),
rmse = NA
)
### TAKES AROUND 4 MINS TO RUN:
for(i in seq_len(nrow(hyper_grid_b4_08))) {
# fit model for ith hyperparameter combination
fit <- randomForest(formula= US ~ ., data = datab4_08, ntree = p * 10,
mtry = hyper_grid_b4_08$mtry[i],
nodesize = hyper_grid_b4_08$nodesize[i],
replace = hyper_grid_b4_08$replace[i],
sampsize = hyper_grid_b4_08$sampsize[i]
)
# save rmse
hyper_grid_b4_08$rmse[i] <- sqrt(fit$mse)
}
hyper_grid_b4_08 <- hyper_grid_b4_08 %>%
arrange(rmse)
head(hyper_grid_b4_08, 10)
